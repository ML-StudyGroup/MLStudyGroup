{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Study Group Prac 1: Single layer and multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Numpy functions for the prac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful Numpy functions (do not run this cell)\n",
    "\n",
    "# dot product (x*y):\n",
    "x.dot(y)\n",
    "\n",
    "# add singleton dimensions (e.g. if the shape of an array is (10,), we can get (10,1)):\n",
    "np.expand_dims\n",
    "\n",
    "# remove singleton dimensions (i.e. if the shape of an array is (10,1), we will then get (10,)):\n",
    "x.squeeze\n",
    "\n",
    "# exponential function (e^x):\n",
    "np.exp\n",
    "\n",
    "# concatenate vectors/matrices side-by-side:\n",
    "np.column_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# activation function choices (you are welcome to implement others too)\n",
    "\n",
    "def signum_activation(x):\n",
    "    return (x >= 0) * 1.\n",
    "\n",
    "def heaviside_activation(x):\n",
    "    return (x >= 0) * 2. - 1\n",
    "\n",
    "def sigmoid_activation(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    else:\n",
    "        return np.exp(x) / (np.exp(x) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate random samples and labels\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "num_features = 2\n",
    "X, Y_true = make_classification(n_features=num_features, n_redundant=0, n_informative=2, n_clusters_per_class=2)\n",
    "X[:, 0] += 3\n",
    "\n",
    "# add bias term\n",
    "X = np.column_stack((X, np.ones(len(X))))\n",
    "\n",
    "# initialise the N+1 weights to random values (the +1 is to include the bias term)\n",
    "\n",
    "# first try without a bias term, then with one\n",
    "num_weights = X.shape[1]\n",
    "W = np.zeros((num_weights, 1))\n",
    "\n",
    "# set learning rate\n",
    "learning_rate = 0.4\n",
    "\n",
    "# learn the weights\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(100):\n",
    "    # IMPLEMENT THE FORWARD (INFERENCE) AND BACKWARD (PARAMETER UPDATE) PASSES HERE\n",
    "    # You can calculate and append the training accuracy too to see how it performs over each epoch\n",
    "    \n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax[0].set_title('True classes')\n",
    "ax[0].scatter(X[:, 0], X[:, 1], marker='o', c=Y_true, s=25, edgecolor='k')\n",
    "ax[0].set_xlabel('x0')\n",
    "ax[0].set_ylabel('x1')\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].set_title('Predicted classes')\n",
    "# ax[1].scatter(X[:, 0], X[:, 1], marker='o', c=Y_pred, s=25, edgecolor='k')\n",
    "ax[1].set_xlabel('x0')\n",
    "ax[1].set_ylabel('x1')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlim([-5, 5])\n",
    "ax[1].set_ylim([-5, 5])\n",
    "\n",
    "# plot accuracy during the course of training\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Accuracy per epoch')\n",
    "plt.plot(accuracies, 'g.-')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "############## PRAC QUESTIONS ###########\n",
    "#\n",
    "# Try to manually find values that separate the two classes well.\n",
    "# Why do we write the decision boundary (in this case a line) as w0 * x0 + w1 * x1 = 0, rather than something like y = m * x + c?\n",
    "# Try including a bias term and see that the algorithm obtains a better fit \n",
    "# Plot the decision boundary. How does its direction relate to the W0, W1 values?\n",
    "# How would you go about plotting the gradient field? (might be easier to do without the bias term first, to keep the number of dimensions lower for visualisation)\n",
    "#\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ PRAC QUESTIONS ##############\n",
    "#\n",
    "# Calculate the derivative of the activation function (e.g. sigmoid)\n",
    "#\n",
    "# For a simple multilayer perceptron model of 2 neurons stacked one after the other (each with 2 inputs), try deriving the \n",
    "# gradient of the error function with respect to the weights. We will use the derivative of the error with respect to each \n",
    "# weight to update that weight's new value.\n",
    "#\n",
    "# Try implement this and test on the same dataset from before. You can also try other datasets.\n",
    "#\n",
    "# Think about how you would handle multiple outputs. i.e. multiply classification classes.\n",
    "# \n",
    "#\n",
    "##########################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
